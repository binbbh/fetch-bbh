{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0fb8d6-0668-47dc-a83c-62162a5f1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8250845-c077-40ba-879d-204627a190e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = pd.read_csv('PRODUCTS.csv')\n",
    "df_user = pd.read_csv('USER.CSV')\n",
    "df_transaction = pd.read_csv('TRANSACTION.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac1b63-99a9-482a-a952-9a45f5f05260",
   "metadata": {},
   "source": [
    "# Let's start off with finding many missing values are in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c517d7e-f5d1-4a21-8d4d-9ce237ba1507",
   "metadata": {},
   "source": [
    "### Creating a function to find the missing value percentage of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32bb344-b334-4eda-8188-3e1eb52881e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_rates(df, df_name):\n",
    "\n",
    "    null_rate = round(df.isnull().mean()*100, 2)\n",
    "    null_rate_sort = null_rate.sort_values(ascending=False)\n",
    "\n",
    "    print(f\"\\n--- Percentage of missing value for the {df_name} table---\")\n",
    "    print(\"{:<15} {:<5}\".format('Column Name', 'Missing Value%'))\n",
    "    for col, null in null_rate_sort.items():\n",
    "        print(\"{:<20} {:<5}%\".format(col, null))\n",
    "    print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05a1f7-9d44-4355-9684-50ed7e082b3d",
   "metadata": {},
   "source": [
    "### Null rate function applied to each table to determine the null rates of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168de938-571b-4659-ac1b-76a3b9c424ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Percentage of missing value for the Products table---\n",
      "Column Name     Missing Value%\n",
      "CATEGORY_4           92.02%\n",
      "MANUFACTURER         26.78%\n",
      "BRAND                26.78%\n",
      "CATEGORY_3           7.16 %\n",
      "BARCODE              0.48 %\n",
      "CATEGORY_2           0.17 %\n",
      "CATEGORY_1           0.01 %\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Percentage of missing value for the User table---\n",
      "Column Name     Missing Value%\n",
      "LANGUAGE             30.51%\n",
      "GENDER               5.89 %\n",
      "STATE                4.81 %\n",
      "BIRTH_DATE           3.68 %\n",
      "ID                   0.0  %\n",
      "CREATED_DATE         0.0  %\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Percentage of missing value for the Transaction table---\n",
      "Column Name     Missing Value%\n",
      "BARCODE              11.52%\n",
      "RECEIPT_ID           0.0  %\n",
      "PURCHASE_DATE        0.0  %\n",
      "SCAN_DATE            0.0  %\n",
      "STORE_NAME           0.0  %\n",
      "USER_ID              0.0  %\n",
      "FINAL_QUANTITY       0.0  %\n",
      "FINAL_SALE           0.0  %\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "null_rates(df_products, 'Products')\n",
    "null_rates(df_user, 'User')\n",
    "null_rates(df_transaction, 'Transaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0cc5a2-77e1-4bec-be28-1edca6846f48",
   "metadata": {},
   "source": [
    "# Based on the results, we notice a discrepancy in the Products table where the barcode is missing for about 0.48% of orders. We also see that the other unique identifiers missing for 26.78% of the values in the brands and manufacturer column. We can take a closer look at that see if there is any correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d4f4d-9bf1-4c11-9e70-66a3a7939588",
   "metadata": {},
   "source": [
    "### Here we're looking at the top 10 offenders of missing barcodes and not find any correlation\n",
    "### the Barcode column with this many missing values is still an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc38d27c-8c21-44d0-af68-958eb80e2268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MANUFACTURER\n",
      "COTY                      269\n",
      "PROCTER & GAMBLE          268\n",
      "NaN                       247\n",
      "GENERAL MILLS             245\n",
      "PEPSICO                   210\n",
      "UNILEVER                  209\n",
      "JOHNSON & JOHNSON         156\n",
      "THE HERSHEY COMPANY       151\n",
      "CHURCH & DWIGHT           142\n",
      "MONDELÄ’Z INTERNATIONAL    140\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{df_products.loc[df_products['BARCODE'].isna(), 'MANUFACTURER'].value_counts(dropna=False).nlargest(10)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68139544-e555-4c8d-bc13-41cfa70c2c4d",
   "metadata": {},
   "source": [
    "### Next we want to validate if the datatype matches with the Entity Relationship Model schema provided in the aws page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f898166-0526-4cb4-97cc-ecfaf63d7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for validating dtypes\n",
    "def validate_dtypes(df, df_name=None):\n",
    "    if df_name is None:\n",
    "        df_name = \"DataFrame\"\n",
    "    print(f\"\\n === Datatypes for {df_name} table ===\")\n",
    "    print(\"{:<15} {:<5}\".format(\"Column Name\", \"Datatype\"))\n",
    "    for col, dt in df.dtypes.items():\n",
    "        # print(\"{:<20} {:<5}\".format(col, dt))\n",
    "        print(f\"{col}: {dt}\")\n",
    "    print('=========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5c9f9-efa3-49d1-ad70-ff56156f86b7",
   "metadata": {},
   "source": [
    "### Validating the dtype of each field which is mostly returning object so we will need to validate most numeric/dt fields\n",
    "### Barcode is already float but can be converted to int but issue can arise if there are any values in the decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "050e572f-e3da-40a6-a24c-be49766cd0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " === Datatypes for Products table ===\n",
      "Column Name     Datatype\n",
      "CATEGORY_1: object\n",
      "CATEGORY_2: object\n",
      "CATEGORY_3: object\n",
      "CATEGORY_4: object\n",
      "MANUFACTURER: object\n",
      "BRAND: object\n",
      "BARCODE: float64\n",
      "=========================================\n",
      "\n",
      " === Datatypes for User table ===\n",
      "Column Name     Datatype\n",
      "ID: object\n",
      "CREATED_DATE: object\n",
      "BIRTH_DATE: object\n",
      "STATE: object\n",
      "LANGUAGE: object\n",
      "GENDER: object\n",
      "=========================================\n",
      "\n",
      " === Datatypes for Transaction table ===\n",
      "Column Name     Datatype\n",
      "RECEIPT_ID: object\n",
      "PURCHASE_DATE: object\n",
      "SCAN_DATE: object\n",
      "STORE_NAME: object\n",
      "USER_ID: object\n",
      "BARCODE: float64\n",
      "FINAL_QUANTITY: object\n",
      "FINAL_SALE: object\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "validate_dtypes(df_products, 'Products')\n",
    "validate_dtypes(df_user, 'User')\n",
    "validate_dtypes(df_transaction, 'Transaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2515712-1510-4ef0-afdb-ce8f9c99c4b4",
   "metadata": {},
   "source": [
    "### Now we can try validating certain columns one by one.\\\n",
    "### We're starting off with date validation, so worked on 2 columns here: scan_date, purchase_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa489236-4adf-4e98-a224-c12bc73591b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date range of Scan Date from Transaction table:\n",
      " min   2024-06-12 06:36:34.910000+00:00\n",
      "max   2024-09-08 23:07:19.836000+00:00\n",
      "Name: SCAN_DATE, dtype: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "df_transaction[\"SCAN_DATE\"] = pd.to_datetime(df_transaction[\"SCAN_DATE\"])\n",
    "scan_date_range = df_transaction[\"SCAN_DATE\"].agg(['min', 'max'])\n",
    "print(f'\\nDate range of Scan Date from Transaction table:\\n {scan_date_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e93e0f1a-6095-4b70-8fa2-d5258f35a5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date range of Purchase Date from Transaction table:\n",
      " min   2024-06-12\n",
      "max   2024-09-08\n",
      "Name: PURCHASE_DATE, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "df_transaction[\"PURCHASE_DATE\"] = pd.to_datetime(df_transaction[\"PURCHASE_DATE\"])\n",
    "purchase_date_range = df_transaction[\"PURCHASE_DATE\"].agg(['min', 'max'])\n",
    "print(f'\\nDate range of Purchase Date from Transaction table:\\n {purchase_date_range}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612e63c-05c5-499f-b614-df4ecc0ec12b",
   "metadata": {},
   "source": [
    "# Here we can see that the scan_date and purchase date matches up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779ec66-0eb0-4cd9-ae77-54b2384c157a",
   "metadata": {},
   "source": [
    "### Next, we will validate the birth_date and created_date from the User table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23649977-6ce5-4298-811a-700b40013fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date range of Birthdays from User table:\n",
      " min   1900-01-01 00:00:00+00:00\n",
      "max   2022-04-03 07:00:00+00:00\n",
      "Name: BIRTH_DATE, dtype: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "df_user[\"BIRTH_DATE\"] = pd.to_datetime(df_user[\"BIRTH_DATE\"])\n",
    "birth_date_range = df_user[\"BIRTH_DATE\"].agg(['min', 'max'])\n",
    "print(f'\\nDate range of Birthdays from User table:\\n {birth_date_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d82ff5f-7423-4ed7-9418-8f1744e76f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date range of Created Date from User table:\n",
      " min   2014-04-18 23:14:55+00:00\n",
      "max   2024-09-11 17:59:15+00:00\n",
      "Name: CREATED_DATE, dtype: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "df_user[\"CREATED_DATE\"] = pd.to_datetime(df_user[\"CREATED_DATE\"])\n",
    "created_date_range = df_user[\"CREATED_DATE\"].agg(['min', 'max'])\n",
    "print(f'\\nDate range of Created Date from User table:\\n {created_date_range}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8dca9-7690-4da2-ba11-02726224ef3b",
   "metadata": {},
   "source": [
    "# Based on the max birth date and min birth date, I am not sure if there should be anyone 2 years old or 124 years old making a purchase.\n",
    "# Also there is a user created on September 11th but made a purchase 3 days prior.\n",
    "# This needs to be verified but can be explained in cases like when a user uses guest checkout but comes back with the same email/phone number to create an account later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289f07b-f27a-4aa2-b90f-c3592af7c71c",
   "metadata": {},
   "source": [
    "### Finally, we can validate the fields with int or float datatype from the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18e5fd07-c526-48a7-89c3-f2c12aaa932a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully converted BARCODE field of Transaction table to INT\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_transaction[\"BARCODE\"] = df_transaction[\"BARCODE\"].dropna().apply(int)\n",
    "    print('successfully converted BARCODE field of Transaction table to INT')\n",
    "except ValueError as e:\n",
    "    print('unable to convert BARCODE field of Transaction table to INT:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95ebb22e-2988-4abf-a6b0-2cf5ba23d8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully converted BARCODE field of Products table to INT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_products[\"BARCODE\"] = df_products[\"BARCODE\"].dropna().apply(int)\n",
    "    print('successfully converted BARCODE field of Products table to INT')\n",
    "except ValueError as e:\n",
    "    print('unable to convert BARCODE field of Products table to INT:', e)\n",
    "print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff0df7-c0a2-4468-b50a-7c0f3c69df55",
   "metadata": {},
   "source": [
    "### We successfully converted the float table to int but not sure if there were any values in the decimal that could have been affected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833f416-7d28-429f-a59b-3c8365971ac0",
   "metadata": {},
   "source": [
    "### Now we can try to validate the numeric fields of quantity and sale of the transaction table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8a946dc-6a84-4a2f-ad97-3049a8aeea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to convert Final_Quantity field of Transaction table to Float: could not convert string to float: 'zero'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_transaction[\"FINAL_QUANTITY\"] = df_transaction[\"FINAL_QUANTITY\"].dropna().apply(float)\n",
    "    print('successfully converted Final_Quantity field of Transaction table to Float')\n",
    "except ValueError as e:\n",
    "    print('unable to convert Final_Quantity field of Transaction table to Float:', e)\n",
    "print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45b593-aa6d-4ddb-a7a3-5faac6269f0e",
   "metadata": {},
   "source": [
    "### The quantity table is unable to convert so we can do a value counts to check it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c39a1b98-56f8-4866-84a0-43089af94ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL_QUANTITY\n",
      "1.00    35698\n",
      "zero    12500\n",
      "2.00     1285\n",
      "3.00      184\n",
      "4.00      139\n",
      "        ...  \n",
      "6.22        1\n",
      "1.22        1\n",
      "1.23        1\n",
      "2.57        1\n",
      "2.27        1\n",
      "Name: count, Length: 87, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_transaction[\"FINAL_QUANTITY\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5539ac-e878-4332-a9f1-005b53b17b86",
   "metadata": {},
   "source": [
    "# This is returning all sorts of values but I am going to assume some can be parts of a measurement like weight.\n",
    "# But the main glaring issue I see here is the \"zero\" value being present.\n",
    "# Not sure how purchases with \"zero\" quantity works but it should at least be a numeric value.\n",
    "# There should a units field in this table if there are quantity that are in the decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d515e19-1713-4912-9f51-bf26664fea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to convert Final_Sale field of Transaction table to Float: could not convert string to float: ' '\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_transaction[\"FINAL_SALE\"] = df_transaction[\"FINAL_SALE\"].dropna().apply(float)\n",
    "    print('successfully converted Final_Sale field of Transaction table to Float')\n",
    "except ValueError as e:\n",
    "    print('unable to convert Final_Sale field of Transaction table to Float:', e)\n",
    "print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3e022-5420-4060-8a9e-d831d14d3e7b",
   "metadata": {},
   "source": [
    "### unable to convert due to empty spaces so lets remove that then try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab403d62-8841-42d3-b385-dcb4da32cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully converted Final_Sale field of Transaction table to Float\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if df_transaction[\"FINAL_SALE\"].astype(str).str.contains(r'[^0-9\\.\\-\\s]').any():\n",
    "        raise ValueError(\"Contains alphabet str\")\n",
    "\n",
    "    # Filling the empty spaces/na values with 0\n",
    "    df_transaction[\"FINAL_SALE\"] = df_transaction[\"FINAL_SALE\"].fillna(0)\n",
    "    df_transaction[\"FINAL_SALE\"] = df_transaction[\"FINAL_SALE\"].astype(str).str.strip()\n",
    "    df_transaction[\"FINAL_SALE\"] = df_transaction[\"FINAL_SALE\"].replace('', '0')\n",
    "    df_transaction[\"FINAL_SALE\"] = df_transaction[\"FINAL_SALE\"].astype(float)\n",
    "    print('successfully converted Final_Sale field of Transaction table to Float')\n",
    "except ValueError as e:\n",
    "    print('unable to convert Final_Sale field of Transaction table to Float:', e)\n",
    "print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9a4e3-fe21-446b-9edc-578ad8d28fbc",
   "metadata": {},
   "source": [
    "### After removing all the na values and empty spaces, we can successfully convert price to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28f367-3e23-4ab6-a338-f501267fcdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
